[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m data analyst with R, SQL, Power BI, and Tableau, experience.\nI’m always eager to explore new topics and challenge myself by finding creative ways to apply my knowledge and skills in various contexts.\nCheck out some of my projects here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bio",
    "section": "",
    "text": "I’m a data analyst with experience in R, SQL, Power BI, and Tableau.\nI’m always eager to explore new topics and challenge myself by finding creative ways to apply my knowledge and skills in various contexts.\nCheck out some of my projects here.",
    "crumbs": [
      "About Me",
      "Bio"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Brighton | Brighton, East Sussex MSc Data Analytics | Oct 2020 - Oct 2021\nUniversity of Leeds | Leeds, West Yorkshire  BA International Development | Sept 2011 - July 2014"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nEnvironment Agency  Analyst | Aug 2023 - present   Leonardo Hotels Revenue Analyst | July 2022 - July 2023"
  },
  {
    "objectID": "about.html#hi-im-tan.",
    "href": "about.html#hi-im-tan.",
    "title": "About",
    "section": "",
    "text": "I’m data analyst with R, SQL, Power BI, and Tableau, experience.\nI’m always eager to explore new topics and challenge myself by finding creative ways to apply my knowledge and skills in various contexts.\nCheck out some of my projects here."
  },
  {
    "objectID": "index.html#hi-im-tan.",
    "href": "index.html#hi-im-tan.",
    "title": "Bio",
    "section": "",
    "text": "I’m a data analyst with experience in R, SQL, Power BI, and Tableau.\nI’m always eager to explore new topics and challenge myself by finding creative ways to apply my knowledge and skills in various contexts.\nCheck out some of my projects here.",
    "crumbs": [
      "About Me",
      "Bio"
    ]
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Bio",
    "section": "Education",
    "text": "Education\nUniversity of Brighton | Brighton, East Sussex MSc Data Analytics | Oct 2020 - Oct 2021\nUniversity of Leeds | Leeds, West Yorkshire  BA International Development | Sept 2011 - July 2014",
    "crumbs": [
      "About Me",
      "Bio"
    ]
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bio",
    "section": "Experience",
    "text": "Experience\nEnvironment Agency | Worthing, West Sussex Analyst | Aug 2023 - present   Leonardo Hotels | Birmingham, West Midlands Revenue Analyst | July 2022 - July 2023",
    "crumbs": [
      "About Me",
      "Bio"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This time series analysis focuses on weekly data available from the Office for National Statistics (ONS). Revolut is one of a number of digital banks that have emerged in recent years and it has around 4.8 million users within the UK financial payment ecosystem. The bank has been providing data to the ONS since the start of the COVID-19 pandemic and the ONS indexes data at the average February 2020 spending level as a pre-pandemic baseline. They also apply a 7-day rolling average to the data to take into account any intra-week spending cyclicality.\nWhile this data is useful for analysing spending changes since 2020 it is important to note a number of limitations:\n\nRevolut customers tend to be younger and more metropolitan than the average UK consumer, so spending may not be representative of the overall UK macroeconomic picture.\nThe indices in the dataset do not take into account inflation and are presented on a nominal basis and are not adjusted for price increases over time. According to the ONS the CPI rate was at 1.8% in January 2020 but by January 2023 was at 10.1%.\nWithin the UK financial transaction ecosystem there has been a shift away from cash as a payment medium in favour of card spending. This results in indices being uplifted over time in areas where consumers replace low value cash transactions with low value card transactions instead. This is more likely to be true in this dataset given the demographic profile of Revolut customers.\n\nThe full background on methodology and limitations is available from the ONS here.",
    "crumbs": [
      "Projects",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "projects/Dashboards/index.html",
    "href": "projects/Dashboards/index.html",
    "title": "Dashboards",
    "section": "",
    "text": "As well as my experience building corporate dashboards in Power BI and Tableau, I’m working on my R Shiny skills and have recently developed a dashboard exploring my parkrun times.\n\nParkrun Dashboard\n\n\n\nPakrun Dashboard\n\n\n\n\n\nPakrun Dashboard"
  },
  {
    "objectID": "projects/Gender_Pay_Gap/index.html",
    "href": "projects/Gender_Pay_Gap/index.html",
    "title": "Gender Pay Gap",
    "section": "",
    "text": "Introduction\nI used data from the UK Gender Pay Gap Service to highlight the gendered disparity in pay between men and women in the UK.\n\n\nMethodology\nThe dataset has almost 49,000 entries across 27 variables for each of the companies that made a submission. I chose to focus on the mean values in difference between hourly and bonus pay across genders for each entry.\nTo do this, I took the relevant mean values and post codes and essentially performed a lookup with a separate dataset of UK postcodes to obtain the local authority names. This dataframe was combined with a shapefile of UK local authorities. I was then able to summarise the data using the mean value in the differences per local authority.\n\n\nLimitations\nThis approach does have limitations, specifically by using mean values per local authority results in data that is not standardised. Neverthless, the intention of this small project was to put together a visualisation rather than conduct an in-depth analysis of the data.\n\n\nR Code\nThe full code is available on GitHub by clicking here.\n\n\n\nUK Paygap Data"
  },
  {
    "objectID": "projects/Time_Series_Analysis/index.html",
    "href": "projects/Time_Series_Analysis/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This time series analysis focuses on weekly data available from the Office for National Statistics (ONS). Revolut is one of a number of digital banks that have emerged in recent years and it has around 4.8 million users within the UK financial payment ecosystem. The bank has been providing data to the ONS since the start of the COVID-19 pandemic and the ONS indexes data at the average February 2020 spending level as a pre-pandemic baseline. They also apply a 7-day rolling average to the data to take into account any intra-week spending cyclicality.\nWhile this data is useful for analysing spending changes since 2020 it is important to note a number of limitations:\n\nRevolut customers tend to be younger and more metropolitan than the average UK consumer, so spending may not be representative of the overall UK macroeconomic picture.\nThe indices in the dataset do not take into account inflation and are presented on a nominal basis and are not adjusted for price increases over time. According to the ONS the CPI rate was at 1.8% in January 2020 but by January 2023 was at 10.1%.\nWithin the UK financial transaction ecosystem there has been a shift away from cash as a payment medium in favour of card spending. This results in indices being uplifted over time in areas where consumers replace low value cash transactions with low value card transactions instead. This is more likely to be true in this dataset given the demographic profile of Revolut customers.\n\nThe full background on methodology and limitations is available from the ONS here."
  },
  {
    "objectID": "projects/Time_Series_Analysis/index.html#revolut-spending-data",
    "href": "projects/Time_Series_Analysis/index.html#revolut-spending-data",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This time series analysis focuses on weekly data available from the Office for National Statistics (ONS). Revolut is one of a number of digital banks that have emerged in recent years and it has around 4.8 million users within the UK financial payment ecosystem. The bank has been providing data to the ONS since the start of the COVID-19 pandemic and the ONS indexes data at the average February 2020 spending level as a pre-pandemic baseline. They also apply a 7-day rolling average to the data to take into account any intra-week spending cyclicality.\nWhile this data is useful for analysing spending changes since 2020 it is important to note a number of limitations:\n\nRevolut customers tend to be younger and more metropolitan than the average UK consumer, so spending may not be representative of the overall UK macroeconomic picture.\nThe indices in the dataset do not take into account inflation and are presented on a nominal basis and are not adjusted for price increases over time. According to the ONS the CPI rate was at 1.8% in January 2020 but by January 2023 was at 10.1%.\nWithin the UK financial transaction ecosystem there has been a shift away from cash as a payment medium in favour of card spending. This results in indices being uplifted over time in areas where consumers replace low value cash transactions with low value card transactions instead. This is more likely to be true in this dataset given the demographic profile of Revolut customers.\n\nThe full background on methodology and limitations is available from the ONS here."
  },
  {
    "objectID": "projects/Time_Series_Analysis/index.html#time-series-analysis",
    "href": "projects/Time_Series_Analysis/index.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nThis project utilizes the indexed rolling 7-day average total spend value from the spending by sector data to identify trends and forecast 28 days ahead. For the analysis, I will use the following methods: exponential smoothing, ARIMA, and forecasting using Prophet.\nAfter some initial cleaning of dataset to ensure it is in the correct format to conduct time series analysis in R, figure 1 shows plot of the spending over time shows how these levels changed over the course of the pandemic.\n\n\n\n\n\n\n\n\n\nIn general, what we can see is a clear upward trend across the three years and some seasonality exhibited around August and December-January. While the initial lockdown might be easily identifiable by the sharp dip during Q1 2020, it may be helpful to provide a reminder of all the restrictions implemented during the pandemic, and the plot below adds further context.\nThe red lines are indicative of the main lockdowns and the restrictions imposed due to the emergence of the Omicron variant in late 2021. Although there were many stages of ‘unlocking’ of restrictions, I have chosen to highlight two key moments with green lines.\n\n\n\n\n\n\n\n\n\nThe first of these easing periods is the Eat Out To Help Out programme during August 2020 which offered discounted meals in pubs, restaurants, and other hospitality outlets to support the sector. The second is in February 2022 when official COVID-19 restrictions were ended under the Living With Covid Plan. As a result of these two interventions, we can clearly identify periods of sustained spending as people had greater freedom of movement in their daily lives.\nIn spite of this, plotting the years against each reveals patterns in the second half of the year, particularly through Q4, that are broadly similar.\n\n\n\n\n\n\n\n\n\n\nExponential Smoothing\n\nHolt’s method with trend\nFor this dataset I have chosen to experiment with the exponential smoothing method with trend and then later add in the seasonality component to determine if this improves the forecast.\n\nfit &lt;- rs3 |&gt;\n  model(\n    `Holt's method` = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\nfabletools::report(fit)\n\nSeries: Total \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9998998 \n    beta  = 0.7845457 \n\n  Initial states:\n     l[0]    b[0]\n 111.1182 2.71086\n\n  sigma^2:  1.4885\n\n     AIC     AICc      BIC \n8771.040 8771.092 8796.381 \n\n\nThis trended exponential smoothing model has a very high alpha value indicating that values in the past are given less weight than more recent values due to the exponential decaying built into the model. In addition, the beta value is high and this takes into changes in the data.\nPlotting the forecast of the next 28 days sees quite a steady decline in the mean value and quite a wide distribution across both the 80% and 95% intervals, with the lower and upper bounds of the 95% interval ranging below 0 and close to 300.\n\n\n\n\n\n\n\n\n\nGiven the macroeconomic conditions in the British economy are have not been particularly positive, this decline might not seem unreasonable and the model may benefit from the introduction of some level of dampening so as not to over-forecast.\n\nfit2 &lt;- rs3 |&gt;\n    model(`Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n    )\nfabletools::report(fit2)\n\nSeries: Total \nModel: ETS(A,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9998987 \n    beta  = 0.8161713 \n    phi   = 0.8000001 \n\n  Initial states:\n     l[0]      b[0]\n 121.2629 -1.124004\n\n  sigma^2:  1.394\n\n     AIC     AICc      BIC \n8694.973 8695.045 8725.382 \n\n\nThe model estimates phi, the dampening coefficient, to be 0.80 and we see a slight decrease in the alpha and increase beta values. This dampened model also performs better compared top the previous model when we compare AIC, AICc, and BIC values. The table below compares the two models and we can see that the damped Holt’s method has a lower RMSE so makes it a better choice.\n\nrs3 |&gt; \n    model(\n        `Holt's method` = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n        `Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n        ) |&gt; accuracy()\n\n# A tibble: 2 × 10\n  .model              .type       ME  RMSE   MAE    MPE  MAPE  MASE RMSSE   ACF1\n  &lt;chr&gt;               &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Holt's method       Trai… -0.00310  1.22 0.728 0.0164 0.742 0.144 0.175 0.0516\n2 Damped Holt's meth… Trai…  0.00414  1.18 0.689 0.0141 0.702 0.136 0.169 0.0136\n\n\nThe forecasts from the damped model also exhibit less variability in the 80% and 95% intervals than the earlier model.\n\n\n\n\n\n\n\n\n\nWhen comparing the two forecasts, the dampened forecast, unsurprisingly, decreases less rapidly or as extreme at the median value.\n\n\n\n\n\n\n\n\n\n\n\n\nHolt-Winters’ method with seasonality\nTo add in a seasonality component to the model we can use the Holt-Winters’ method either with an additive seasonality component, which assumes the seasonal variations within the time series to be approximately constant, or multiplicative seasonality component, where the assumption is that the variations are proportional to the level of the series.\nWith the initial modelling, we can see that the additive model performs better with its lower AIC, AICc, and BIC values, in addition to lower MSE and AMSE values.\n\n\n# A tibble: 2 × 9\n  .model           sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE    MAE\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Additive       1.78      -4483. 8990. 8990. 9051.  1.77  7.62 0.811 \n2 Multiplicative 0.000265  -4674. 9371. 9371. 9432.  2.83  9.74 0.0104\n\n\nThe difference between the two models is evident when forecasting the next 28 days, with the mean values for additive model increasing while the multiplicative model shows a decline.\n\n\n\n\n\n\n\n\n\nSimilarly, the RMSE for the additive model is better than multiplicative one, although it does not perform as well as the RMSE for the damped Holt’s method above.\n\n\n# A tibble: 2 × 10\n  .model         .type           ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;          &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Additive       Training  0.000715  1.33 0.811 0.0304 0.827 0.161 0.191 0.323\n2 Multiplicative Training -0.00684   1.68 1.03  0.0278 1.04  0.203 0.242 0.374\n\n\nHowever, it is possible to combine the dampened trend method with both additive and multiplicative seasonality. Both models see a drop in MSE and the AIC, AICc, and BIC values fall significantly, with the dampened multiplicative Holt Winter’s method having very similar values to the dampened Holt’s method. The RMSE value improves but not to the extent that either model match the model using damped Holt’s method.\n\n\n# A tibble: 2 × 9\n  .model           sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE     MAE\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Additive       1.57      -4406. 8839. 8839. 8905.  1.55  6.04 0.749  \n2 Multiplicative 0.000157  -4368. 8762. 8762. 8827.  1.66  6.78 0.00786\n\n\n\n\n# A tibble: 2 × 10\n  .model         .type          ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Additive       Training 0.00681   1.25 0.749 0.0165 0.761 0.148 0.179 0.327\n2 Multiplicative Training 0.000218  1.29 0.778 0.0210 0.788 0.154 0.185 0.324\n\n\nThe plot here of the 28 day ahead forecast exhibits shallower increases for both models and a narrower set of intervals at 80% and 95%.\n\n\n\n\n\n\n\n\n\n\nModel selection\nHaving previously worked through a number of combinations of exponential smoothing models, both with and without dampening or seasonality, it seems that the damped Holt’s method without seasonality performed best when comparing AICc and RMSE values. However, it’s possible to employ the ETS() function in R to generate a model that minimises the AICc value.\n\n\nSeries: Total \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.7938542 \n    phi   = 0.8000637 \n\n  Initial states:\n     l[0]      b[0]\n 116.3569 -5.895034\n\n  sigma^2:  1e-04\n\n     AIC     AICc      BIC \n8529.853 8529.925 8560.262 \n\n\nComparisons between this preferred model and the damped Holt’s model from earlier are below:\n\n\n# A tibble: 2 × 9\n  .model                 sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE     MAE\n  &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 R Preferred          0.000130  -4259. 8530. 8530. 8560.  1.33  6.13 0.00697\n2 Damped Holt's method 1.39      -4341. 8695. 8695. 8725.  1.39  6.17 0.689  \n\n\n\n\n# A tibble: 2 × 10\n  .model               .type      ME  RMSE   MAE    MPE  MAPE  MASE RMSSE   ACF1\n  &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 R Preferred          Trai… 0.00912  1.15 0.684 0.0186 0.698 0.135 0.166 0.0745\n2 Damped Holt's method Trai… 0.00414  1.18 0.689 0.0141 0.702 0.136 0.169 0.0136\n\n\nThe output above tells us the model preferred by R uses Holt’s method where the error is multiplicative (rather than additive as has been used in the examples above), with an additive trend that is dampened. The value for alpha is very high, reflecting how the weight of the past observations decays quite rapidly. The AICc value is the lowest of all the models we have tested above.\nThe forecast of the next 28 days reflects the marginal decline exhibited in some of the other dampened models above. Given that the alpha value is relatively high it is not unsurprising that this model forecasts the mean value to be close to the total 7-day average spend in 2023.\n\n\n\n\n\n\n\n\n\nWe can use cross-validation based on a rolling forecasting origin, starting at the end of 2022 (chosen due to the impact of the pandemic prior to that) and increasing by one step each time. The result of this process is that the model preferred by R performs the best both when comparing AICc and RMSE.\n\n\n# A tibble: 2 × 9\n  .model                 sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE     MAE\n  &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 R Preferred          0.000130  -4259. 8530. 8530. 8560.  1.33  6.13 0.00697\n2 Damped Holt's method 1.39      -4341. 8695. 8695. 8725.  1.39  6.17 0.689  \n\n\n\n\n# A tibble: 2 × 10\n  .model               .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;                &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Damped Holt's method Test  0.841  9.92  7.73 0.492  5.58  1.53  1.43 0.979\n2 R Preferred          Test  0.912  9.77  7.66 0.544  5.53  1.52  1.40 0.979\n\n\nChecking the residuals of R’s preferred model, we can see that the innovation residuals appear to have a constant variance and mean of zero. The histogram exhibits some degree of normality although the peak is a little high. The ACF plot have a number of significant that decay exponentially.\n\n\n\n\n\n\n\n\n\nIn contrast to the damped Holt’s method model, while the residuals have a mean of zero, the variance does appear to increase as we reach the end of 2022 and start 2023. The histogram is slightly left-skewed and peaks so may also fail the normality test. The ACF plot bears some similarity to R’s preferred model with a number of significant lags and before decaying. Given this, R’s preferred model seems to be the better model.\n\n\n\n\n\n\n\n\n\n\n\n\nARIMA modelling\nLooking at the initial plot of data above, it is clear that the data is not stationary and the ACF plot below shows the same data when it is not differenced up to lag 100. The lags remain significant are taking a long time to decay because each is correlated to the previous one. This makes sense when we consider that each daily value in the dataset relates to rolling 7-day average spend so we would expect correlation between the current and previous values.\n\n\n\n\n\n\n\n\n\nTo make the data stationary, I have had to apply a log transformation to stabilise the variance and first order differencing to stabilise the mean. The resulting plot has a number of significant spikes around the time of lockdown restrictions being introduced, but overall resembles white noise.\n\n\n\n\n\n\n\n\n\nThe differenced ACF plot exhibits significant lags up to lag 6 before decaying away.\n\n\n\n\n\n\n\n\n\nIt is also possible to confirm stationarity of this differenced data with a KPSS unit root test, which gives a small test statistic and a p-value of 0.1 and allows us to assume the data is stationary:\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.139         0.1\n\n\nThe time plot and ACF & PACF plots of the stationary data are show below.\n\n\n\n\n\n\n\n\n\nGiven the data is a 7-day rolling average and that the ACF plot appears to show a sinusodial and seasonal pattern of aproximately 7 days, a seasonal ARIMA model would be appropriate. In order to achieve this, I will let R try to find the best model order. The first is the default stepwise procedure and the second one works harder to search for a better model.\n\nar_fit &lt;- rs3 |&gt; model(stepwise = ARIMA(Total),\n             search = ARIMA(Total, stepwise=FALSE, approx = FALSE))\n\nAs we can from the output here, R has ARIMA(3,1,0)(1,0,1)7 for both models\n\n\n# A mable: 2 x 2\n# Key:     Model name [2]\n  `Model name`                   Orders\n  &lt;chr&gt;                         &lt;model&gt;\n1 stepwise     &lt;ARIMA(3,1,0)(1,0,1)[7]&gt;\n2 search       &lt;ARIMA(3,1,0)(1,0,1)[7]&gt;\n\n\n\n\n# A tibble: 2 × 6\n  .model   sigma2 log_lik   AIC  AICc   BIC\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 stepwise  0.955  -1640. 3292. 3292. 3322.\n2 search    0.955  -1640. 3292. 3292. 3322.\n\n\nThe residuals, as shown below, appear to have a constant mean and variance in the time plot and the ACF, whilst show some spikes appears consistent with white noise. However, the model fails the Ljung-Box text for white noise. This model is still left-skewed and peaks a little too high in the histogram, thus failing the normality test.\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 search    133.   0.00420\n\n\nAlthough this model does not pass all of the residual tests we can still use it to forecast, bearing in mind the limitations concerning the accuracy of prediction intervals. Forecasts for the next 28 days are shown below.\n\n\n\n\n\n\n\n\n\n\n\nProphet\nFor an alternative approach, I have chosen to use Meta’s Prophet tool for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\nIn order to use package, the dataset had to be modified so the date variable was renamed ‘ds’ and the total variable as ‘y’. In addition, Prophet allows for custom holidays to be added to the model to take into account national holidays that occur. The holidays function can also be used to deal with systemic shocks that would impact a time series so as to prevent the trend component capturing any peaks or troughs in the data. As such, I have created an additional tibble that holds the dates for the three UK lockdowns and the period of restrictions in place following the emergence of the omicron variant of COVID-19.\n\n\n# A tsibble: 5 x 2 [1D]\n  ds             y\n  &lt;date&gt;     &lt;dbl&gt;\n1 2020-01-01  114.\n2 2020-01-02  118.\n3 2020-01-03  118.\n4 2020-01-04  116.\n5 2020-01-05  114.\n\n\n\n\n# A tibble: 4 × 5\n  holiday    ds         lower_window ds_upper   upper_window\n  &lt;chr&gt;      &lt;date&gt;            &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 lockdown_1 2020-03-26            0 2020-05-11            0\n2 lockdown_2 2020-11-05            0 2020-12-02            0\n3 lockdown_3 2021-01-05            0 2021-05-17            0\n4 omicron    2021-12-08            0 2022-02-24            0\n\n\nWith the data set up to, I started by modelling the time series with limited changes to the available parameters. However, the changepoint_prior_scale parameter was increased to make the trend more flexible and the seasonality mode changed from additive to multiplicative.\n\nm &lt;- prophet(df, changepoint.prior.scale = 0.3, holidays = lockdowns, seasonality.mode = \"multiplicative\")\n\nfuture &lt;- make_future_dataframe(m, periods = 28)\n\nforecast &lt;- predict(m, future)\n\nplot(m, forecast)+\n    labs(title = \"Prophet multiplicative model\",\n         y=\"Spend (£)\",\n        x=\"Date\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n\n\n\n\n\n\n\n\nThis interactive plot allows for the model to be viewed in more detail:\n\n\n\n\n\n\nUsing Prophet we are able to decompose the model and look at each component separately.\n\n\n\n\n\n\n\n\n\nFinally, using cross-validation we can measure forecast error against historical data in a method akin to a rolling forecast origin used earlier. With Prophet I have chosen to select an initial period of 400 days (i.e. up to early February 2021) and made predictions every 90 days for a forecast horizon of 28 days. Based on the current time series, this corresponded to 9 forecasts.\n\ndf.cv &lt;- cross_validation(m, initial = 400, period = 90, horizon = 28, units = 'days')\n\nOnce computed, we can visualise a range of statistics of prediction performance. In the plot below, the dots show the absolute percentage error and the blue line the mean absolute percentage error over the forecast horizon. Forecast error here remains up to 10% for the first 12 days but then steadily increases to a maximum of around 15% for predictions 28 days out.\n\n\n  horizon       mse      rmse       mae       mape      mdape      smape\n1  3 days  41.00205  6.403284  5.424640 0.05005081 0.04513999 0.05071951\n2  4 days  54.55002  7.385799  6.271378 0.05666009 0.04609069 0.05749919\n3  5 days  67.05754  8.188867  7.034154 0.06205384 0.05589260 0.06309027\n4  6 days  83.74800  9.151393  7.977976 0.06876534 0.06562857 0.07001672\n5  7 days 113.42346 10.650045  9.379767 0.07910440 0.07579380 0.08061358\n6  8 days 148.41528 12.182581 10.719817 0.08878131 0.08017642 0.09073356\n   coverage\n1 0.4355556\n2 0.3644444\n3 0.2933333\n4 0.2133333\n5 0.1422222\n6 0.1111111"
  },
  {
    "objectID": "projects1.html",
    "href": "projects1.html",
    "title": "R Shiny Dashboard",
    "section": "",
    "text": "As well as my experience building corporate dashboards in Power BI and Tableau, I’m working on my R Shiny skills and have recently developed a dashboard exploring my parkrun times.\n\nParkrun Dashboard\n\n\n\nPakrun Dashboard\n\n\n\n\n\nPakrun Dashboard",
    "crumbs": [
      "Projects",
      "R Shiny Dashboard"
    ]
  },
  {
    "objectID": "projects2.html",
    "href": "projects2.html",
    "title": "Gender Pay Gap Map",
    "section": "",
    "text": "Introduction\nI used data from the UK Gender Pay Gap Service to highlight the gendered disparity in pay between men and women in the UK.\n\n\nMethodology\nThe dataset has almost 49,000 entries across 27 variables for each of the companies that made a submission. I chose to focus on the mean values in difference between hourly and bonus pay across genders for each entry.\nTo do this, I took the relevant mean values and post codes and essentially performed a lookup with a separate dataset of UK postcodes to obtain the local authority names. This dataframe was combined with a shapefile of UK local authorities. I was then able to summarise the data using the mean value in the differences per local authority.\n\n\nLimitations\nThis approach does have limitations, specifically by using mean values per local authority results in data that is not standardised. Neverthless, the intention of this small project was to put together a visualisation rather than conduct an in-depth analysis of the data.\n\n\nR Code\nThe full code is available on GitHub by clicking here.",
    "crumbs": [
      "Projects",
      "Gender Pay Gap Map"
    ]
  },
  {
    "objectID": "projects.html#revolut-spending-data",
    "href": "projects.html#revolut-spending-data",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This time series analysis focuses on weekly data available from the Office for National Statistics (ONS). Revolut is one of a number of digital banks that have emerged in recent years and it has around 4.8 million users within the UK financial payment ecosystem. The bank has been providing data to the ONS since the start of the COVID-19 pandemic and the ONS indexes data at the average February 2020 spending level as a pre-pandemic baseline. They also apply a 7-day rolling average to the data to take into account any intra-week spending cyclicality.\nWhile this data is useful for analysing spending changes since 2020 it is important to note a number of limitations:\n\nRevolut customers tend to be younger and more metropolitan than the average UK consumer, so spending may not be representative of the overall UK macroeconomic picture.\nThe indices in the dataset do not take into account inflation and are presented on a nominal basis and are not adjusted for price increases over time. According to the ONS the CPI rate was at 1.8% in January 2020 but by January 2023 was at 10.1%.\nWithin the UK financial transaction ecosystem there has been a shift away from cash as a payment medium in favour of card spending. This results in indices being uplifted over time in areas where consumers replace low value cash transactions with low value card transactions instead. This is more likely to be true in this dataset given the demographic profile of Revolut customers.\n\nThe full background on methodology and limitations is available from the ONS here.",
    "crumbs": [
      "Projects",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "projects.html#time-series-analysis",
    "href": "projects.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nThis project utilizes the indexed rolling 7-day average total spend value from the spending by sector data to identify trends and forecast 28 days ahead. For the analysis, I will use the following methods: exponential smoothing, ARIMA, and forecasting using Prophet.\nAfter some initial cleaning of dataset to ensure it is in the correct format to conduct time series analysis in R, figure 1 shows plot of the spending over time shows how these levels changed over the course of the pandemic.\n\n\n\n\n\n\n\n\n\nIn general, what we can see is a clear upward trend across the three years and some seasonality exhibited around August and December-January. While the initial lockdown might be easily identifiable by the sharp dip during Q1 2020, it may be helpful to provide a reminder of all the restrictions implemented during the pandemic, and the plot below adds further context.\nThe red lines are indicative of the main lockdowns and the restrictions imposed due to the emergence of the Omicron variant in late 2021. Although there were many stages of ‘unlocking’ of restrictions, I have chosen to highlight two key moments with green lines.\n\n\n\n\n\n\n\n\n\nThe first of these easing periods is the Eat Out To Help Out programme during August 2020 which offered discounted meals in pubs, restaurants, and other hospitality outlets to support the sector. The second is in February 2022 when official COVID-19 restrictions were ended under the Living With Covid Plan. As a result of these two interventions, we can clearly identify periods of sustained spending as people had greater freedom of movement in their daily lives.\nIn spite of this, plotting the years against each reveals patterns in the second half of the year, particularly through Q4, that are broadly similar.\n\n\n\n\n\n\n\n\n\n\nExponential Smoothing\n\nHolt’s method with trend\nFor this dataset I have chosen to experiment with the exponential smoothing method with trend and then later add in the seasonality component to determine if this improves the forecast.\n\nfit &lt;- rs3 |&gt;\n  model(\n    `Holt's method` = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\nfabletools::report(fit)\n\nSeries: Total \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9998998 \n    beta  = 0.7845457 \n\n  Initial states:\n     l[0]    b[0]\n 111.1182 2.71086\n\n  sigma^2:  1.4885\n\n     AIC     AICc      BIC \n8771.040 8771.092 8796.381 \n\n\nThis trended exponential smoothing model has a very high alpha value indicating that values in the past are given less weight than more recent values due to the exponential decaying built into the model. In addition, the beta value is high and this takes into changes in the data.\nPlotting the forecast of the next 28 days sees quite a steady decline in the mean value and quite a wide distribution across both the 80% and 95% intervals, with the lower and upper bounds of the 95% interval ranging below 0 and close to 300.\n\n\n\n\n\n\n\n\n\nGiven the macroeconomic conditions in the British economy are have not been particularly positive, this decline might not seem unreasonable and the model may benefit from the introduction of some level of dampening so as not to over-forecast.\n\nfit2 &lt;- rs3 |&gt;\n    model(`Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n    )\nfabletools::report(fit2)\n\nSeries: Total \nModel: ETS(A,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9998987 \n    beta  = 0.8161713 \n    phi   = 0.8000001 \n\n  Initial states:\n     l[0]      b[0]\n 121.2629 -1.124004\n\n  sigma^2:  1.394\n\n     AIC     AICc      BIC \n8694.973 8695.045 8725.382 \n\n\nThe model estimates phi, the dampening coefficient, to be 0.80 and we see a slight decrease in the alpha and increase beta values. This dampened model also performs better compared top the previous model when we compare AIC, AICc, and BIC values. The table below compares the two models and we can see that the damped Holt’s method has a lower RMSE so makes it a better choice.\n\nrs3 |&gt; \n    model(\n        `Holt's method` = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n        `Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n        ) |&gt; accuracy()\n\n# A tibble: 2 × 10\n  .model              .type       ME  RMSE   MAE    MPE  MAPE  MASE RMSSE   ACF1\n  &lt;chr&gt;               &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Holt's method       Trai… -0.00310  1.22 0.728 0.0164 0.742 0.144 0.175 0.0516\n2 Damped Holt's meth… Trai…  0.00414  1.18 0.689 0.0141 0.702 0.136 0.169 0.0136\n\n\nThe forecasts from the damped model also exhibit less variability in the 80% and 95% intervals than the earlier model.\n\n\n\n\n\n\n\n\n\nWhen comparing the two forecasts, the dampened forecast, unsurprisingly, decreases less rapidly or as extreme at the median value.\n\n\n\n\n\n\n\n\n\n\n\n\nHolt-Winters’ method with seasonality\nTo add in a seasonality component to the model we can use the Holt-Winters’ method either with an additive seasonality component, which assumes the seasonal variations within the time series to be approximately constant, or multiplicative seasonality component, where the assumption is that the variations are proportional to the level of the series.\nWith the initial modelling, we can see that the additive model performs better with its lower AIC, AICc, and BIC values, in addition to lower MSE and AMSE values.\n\n\n# A tibble: 2 × 9\n  .model           sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE    MAE\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Additive       1.78      -4483. 8990. 8990. 9051.  1.77  7.62 0.811 \n2 Multiplicative 0.000265  -4674. 9371. 9371. 9432.  2.83  9.74 0.0104\n\n\nThe difference between the two models is evident when forecasting the next 28 days, with the mean values for additive model increasing while the multiplicative model shows a decline.\n\n\n\n\n\n\n\n\n\nSimilarly, the RMSE for the additive model is better than multiplicative one, although it does not perform as well as the RMSE for the damped Holt’s method above.\n\n\n# A tibble: 2 × 10\n  .model         .type           ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;          &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Additive       Training  0.000715  1.33 0.811 0.0304 0.827 0.161 0.191 0.323\n2 Multiplicative Training -0.00684   1.68 1.03  0.0278 1.04  0.203 0.242 0.374\n\n\nHowever, it is possible to combine the dampened trend method with both additive and multiplicative seasonality. Both models see a drop in MSE and the AIC, AICc, and BIC values fall significantly, with the dampened multiplicative Holt Winter’s method having very similar values to the dampened Holt’s method. The RMSE value improves but not to the extent that either model match the model using damped Holt’s method.\n\n\n# A tibble: 2 × 9\n  .model           sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE     MAE\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Additive       1.57      -4406. 8839. 8839. 8905.  1.55  6.04 0.749  \n2 Multiplicative 0.000157  -4368. 8762. 8762. 8827.  1.66  6.78 0.00786\n\n\n\n\n# A tibble: 2 × 10\n  .model         .type          ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Additive       Training 0.00681   1.25 0.749 0.0165 0.761 0.148 0.179 0.327\n2 Multiplicative Training 0.000218  1.29 0.778 0.0210 0.788 0.154 0.185 0.324\n\n\nThe plot here of the 28 day ahead forecast exhibits shallower increases for both models and a narrower set of intervals at 80% and 95%.\n\n\n\n\n\n\n\n\n\n\nModel selection\nHaving previously worked through a number of combinations of exponential smoothing models, both with and without dampening or seasonality, it seems that the damped Holt’s method without seasonality performed best when comparing AICc and RMSE values. However, it’s possible to employ the ETS() function in R to generate a model that minimises the AICc value.\n\n\nSeries: Total \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.7938542 \n    phi   = 0.8000637 \n\n  Initial states:\n     l[0]      b[0]\n 116.3569 -5.895034\n\n  sigma^2:  1e-04\n\n     AIC     AICc      BIC \n8529.853 8529.925 8560.262 \n\n\nComparisons between this preferred model and the damped Holt’s model from earlier are below:\n\n\n# A tibble: 2 × 9\n  .model                 sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE     MAE\n  &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 R Preferred          0.000130  -4259. 8530. 8530. 8560.  1.33  6.13 0.00697\n2 Damped Holt's method 1.39      -4341. 8695. 8695. 8725.  1.39  6.17 0.689  \n\n\n\n\n# A tibble: 2 × 10\n  .model               .type      ME  RMSE   MAE    MPE  MAPE  MASE RMSSE   ACF1\n  &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 R Preferred          Trai… 0.00912  1.15 0.684 0.0186 0.698 0.135 0.166 0.0745\n2 Damped Holt's method Trai… 0.00414  1.18 0.689 0.0141 0.702 0.136 0.169 0.0136\n\n\nThe output above tells us the model preferred by R uses Holt’s method where the error is multiplicative (rather than additive as has been used in the examples above), with an additive trend that is dampened. The value for alpha is very high, reflecting how the weight of the past observations decays quite rapidly. The AICc value is the lowest of all the models we have tested above.\nThe forecast of the next 28 days reflects the marginal decline exhibited in some of the other dampened models above. Given that the alpha value is relatively high it is not unsurprising that this model forecasts the mean value to be close to the total 7-day average spend in 2023.\n\n\n\n\n\n\n\n\n\nWe can use cross-validation based on a rolling forecasting origin, starting at the end of 2022 (chosen due to the impact of the pandemic prior to that) and increasing by one step each time. The result of this process is that the model preferred by R performs the best both when comparing AICc and RMSE.\n\n\n# A tibble: 2 × 9\n  .model                 sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE     MAE\n  &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 R Preferred          0.000130  -4259. 8530. 8530. 8560.  1.33  6.13 0.00697\n2 Damped Holt's method 1.39      -4341. 8695. 8695. 8725.  1.39  6.17 0.689  \n\n\n\n\n# A tibble: 2 × 10\n  .model               .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;                &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Damped Holt's method Test  0.841  9.92  7.73 0.492  5.58  1.53  1.43 0.979\n2 R Preferred          Test  0.912  9.77  7.66 0.544  5.53  1.52  1.40 0.979\n\n\nChecking the residuals of R’s preferred model, we can see that the innovation residuals appear to have a constant variance and mean of zero. The histogram exhibits some degree of normality although the peak is a little high. The ACF plot have a number of significant that decay exponentially.\n\n\n\n\n\n\n\n\n\nIn contrast to the damped Holt’s method model, while the residuals have a mean of zero, the variance does appear to increase as we reach the end of 2022 and start 2023. The histogram is slightly left-skewed and peaks so may also fail the normality test. The ACF plot bears some similarity to R’s preferred model with a number of significant lags and before decaying. Given this, R’s preferred model seems to be the better model.\n\n\n\n\n\n\n\n\n\n\n\n\nARIMA modelling\nLooking at the initial plot of data above, it is clear that the data is not stationary and the ACF plot below shows the same data when it is not differenced up to lag 100. The lags remain significant are taking a long time to decay because each is correlated to the previous one. This makes sense when we consider that each daily value in the dataset relates to rolling 7-day average spend so we would expect correlation between the current and previous values.\n\n\n\n\n\n\n\n\n\nTo make the data stationary, I have had to apply a log transformation to stabilise the variance and first order differencing to stabilise the mean. The resulting plot has a number of significant spikes around the time of lockdown restrictions being introduced, but overall resembles white noise.\n\n\n\n\n\n\n\n\n\nThe differenced ACF plot exhibits significant lags up to lag 6 before decaying away.\n\n\n\n\n\n\n\n\n\nIt is also possible to confirm stationarity of this differenced data with a KPSS unit root test, which gives a small test statistic and a p-value of 0.1 and allows us to assume the data is stationary:\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.139         0.1\n\n\nThe time plot and ACF & PACF plots of the stationary data are show below.\n\n\n\n\n\n\n\n\n\nGiven the data is a 7-day rolling average and that the ACF plot appears to show a sinusodial and seasonal pattern of aproximately 7 days, a seasonal ARIMA model would be appropriate. In order to achieve this, I will let R try to find the best model order. The first is the default stepwise procedure and the second one works harder to search for a better model.\n\nar_fit &lt;- rs3 |&gt; model(stepwise = ARIMA(Total),\n             search = ARIMA(Total, stepwise=FALSE, approx = FALSE))\n\nAs we can from the output here, R has ARIMA(3,1,0)(1,0,1)7 for both models\n\n\n# A mable: 2 x 2\n# Key:     Model name [2]\n  `Model name`                   Orders\n  &lt;chr&gt;                         &lt;model&gt;\n1 stepwise     &lt;ARIMA(3,1,0)(1,0,1)[7]&gt;\n2 search       &lt;ARIMA(3,1,0)(1,0,1)[7]&gt;\n\n\n\n\n# A tibble: 2 × 6\n  .model   sigma2 log_lik   AIC  AICc   BIC\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 stepwise  0.955  -1640. 3292. 3292. 3322.\n2 search    0.955  -1640. 3292. 3292. 3322.\n\n\nThe residuals, as shown below, appear to have a constant mean and variance in the time plot and the ACF, whilst show some spikes appears consistent with white noise. However, the model fails the Ljung-Box text for white noise. This model is still left-skewed and peaks a little too high in the histogram, thus failing the normality test.\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 search    133.   0.00420\n\n\nAlthough this model does not pass all of the residual tests we can still use it to forecast, bearing in mind the limitations concerning the accuracy of prediction intervals. Forecasts for the next 28 days are shown below.\n\n\n\n\n\n\n\n\n\n\n\nProphet\nFor an alternative approach, I have chosen to use Meta’s Prophet tool for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\nIn order to use package, the dataset had to be modified so the date variable was renamed ‘ds’ and the total variable as ‘y’. In addition, Prophet allows for custom holidays to be added to the model to take into account national holidays that occur. The holidays function can also be used to deal with systemic shocks that would impact a time series so as to prevent the trend component capturing any peaks or troughs in the data. As such, I have created an additional tibble that holds the dates for the three UK lockdowns and the period of restrictions in place following the emergence of the omicron variant of COVID-19.\n\n\n# A tsibble: 5 x 2 [1D]\n  ds             y\n  &lt;date&gt;     &lt;dbl&gt;\n1 2020-01-01  114.\n2 2020-01-02  118.\n3 2020-01-03  118.\n4 2020-01-04  116.\n5 2020-01-05  114.\n\n\n\n\n# A tibble: 4 × 5\n  holiday    ds         lower_window ds_upper   upper_window\n  &lt;chr&gt;      &lt;date&gt;            &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 lockdown_1 2020-03-26            0 2020-05-11            0\n2 lockdown_2 2020-11-05            0 2020-12-02            0\n3 lockdown_3 2021-01-05            0 2021-05-17            0\n4 omicron    2021-12-08            0 2022-02-24            0\n\n\nWith the data set up to, I started by modelling the time series with limited changes to the available parameters. However, the changepoint_prior_scale parameter was increased to make the trend more flexible and the seasonality mode changed from additive to multiplicative.\n\nm &lt;- prophet(df, changepoint.prior.scale = 0.3, holidays = lockdowns, seasonality.mode = \"multiplicative\")\n\nfuture &lt;- make_future_dataframe(m, periods = 28)\n\nforecast &lt;- predict(m, future)\n\nplot(m, forecast)+\n    labs(title = \"Prophet multiplicative model\",\n         y=\"Spend (£)\",\n        x=\"Date\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n\n\n\n\n\n\n\n\nThis interactive plot allows for the model to be viewed in more detail:\n\n\n\n\n\n\nUsing Prophet we are able to decompose the model and look at each component separately.\n\n\n\n\n\n\n\n\n\nFinally, using cross-validation we can measure forecast error against historical data in a method akin to a rolling forecast origin used earlier. With Prophet I have chosen to select an initial period of 400 days (i.e. up to early February 2021) and made predictions every 90 days for a forecast horizon of 28 days. Based on the current time series, this corresponded to 9 forecasts.\n\ndf.cv &lt;- cross_validation(m, initial = 400, period = 90, horizon = 28, units = 'days')\n\nOnce computed, we can visualise a range of statistics of prediction performance. In the plot below, the dots show the absolute percentage error and the blue line the mean absolute percentage error over the forecast horizon. Forecast error here remains up to 10% for the first 12 days but then steadily increases to a maximum of around 15% for predictions 28 days out.\n\n\n  horizon       mse      rmse       mae       mape      mdape      smape\n1  3 days  41.00205  6.403284  5.424640 0.05005081 0.04513999 0.05071951\n2  4 days  54.55002  7.385799  6.271378 0.05666009 0.04609069 0.05749919\n3  5 days  67.05754  8.188867  7.034154 0.06205384 0.05589260 0.06309027\n4  6 days  83.74800  9.151393  7.977976 0.06876534 0.06562857 0.07001672\n5  7 days 113.42346 10.650045  9.379767 0.07910440 0.07579380 0.08061358\n6  8 days 148.41528 12.182581 10.719817 0.08878131 0.08017642 0.09073356\n   coverage\n1 0.4044444\n2 0.3644444\n3 0.2933333\n4 0.2133333\n5 0.1422222\n6 0.1111111",
    "crumbs": [
      "Projects",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "How to pivot datasets\n\n\nA guide to using pivot_longer() and pivot_wider() in {tidyr}\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference",
      "Guides"
    ]
  },
  {
    "objectID": "posts/08-06-2024-pivoting/index.html",
    "href": "posts/08-06-2024-pivoting/index.html",
    "title": "How to pivot datasets",
    "section": "",
    "text": "Why, oh why?\nI’ve spent quite a few years working with data in R but was reminded of the brain fog that overcomes me each time I want to pivot any data when I saw Jörn’s post on Bluesky:\nWill ever I be able to convert data between wide and long format in #rstats without googling?— Jörn Alexander Quent (@jaquent.bsky.social) Jun 6, 2024 at 7:40\nI regularly use Excel and Power BI at work and pivoting can be pretty simple in Power Query (here’s the answer in case you’re wondering) but this site is a Microsoft-free zone so I’ll show you how to do in RStudio.\n\n\nEnter the Tidyverse\nFor this guide we’re using the {tidyr} package from {tidyverse} so let’s get them loaded up and set up a dataset for use in this guide:\n\nlibrary(tidyverse)\n\n\nindex &lt;- seq(1:10)\np1 &lt;- runif(10,1,50)\np2 &lt;- runif(10,5,70)\np3 &lt;- runif(10,7,90)\ndf &lt;- tibble(index,p1,p2,p3)\nprint(df)\n\n# A tibble: 10 × 4\n   index    p1    p2    p3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 44.4   66.3  60.5\n 2     2 25.0   29.9  73.2\n 3     3  4.50  69.8  34.7\n 4     4 12.3   46.0  13.0\n 5     5 32.1   20.2  78.4\n 6     6 41.9   62.4  22.2\n 7     7 13.1   41.9  52.8\n 8     8 18.3   33.7  33.7\n 9     9 16.5   52.8  37.5\n10    10 26.5   63.3  28.3\n\n\nNow, let’s say we want to everything apart from the index column to be reshaped into a longer tibble where the p labels are in a column called class and the actual observations for each row are in a column called values. Using the pivot_longer() function we could do something like this were we select everything apart from index and column names are assigned using the names_to argument and values using values_to:\n\ndf |&gt; \n  pivot_longer(!index, \n               names_to = \"class\", \n               values_to = \"value\")\n\n# A tibble: 30 × 3\n   index class value\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 p1    44.4 \n 2     1 p2    66.3 \n 3     1 p3    60.5 \n 4     2 p1    25.0 \n 5     2 p2    29.9 \n 6     2 p3    73.2 \n 7     3 p1     4.50\n 8     3 p2    69.8 \n 9     3 p3    34.7 \n10     4 p1    12.3 \n# ℹ 20 more rows\n\n\nOther ways of achieving this same result are by selecting specific columns from their column index; {tidyselect}’s selection helpers; or a regexp using the names_pattern argument:\n\n# Use column index to select columns\ndf |&gt; \n  pivot_longer(cols = c(2:4),\n               names_to = \"class\",\n               values_to = \"values\")\n\n\n# Use starts_with to select columns\ndf |&gt; \n  pivot_longer(cols = starts_with(\"p\"),\n               names_to = \"class\",\n               values_to = \"values\")\n\n# Use a regexp to select columns\ndf |&gt; \n  pivot_longer(cols = c(2:4),\n               names_to = \"class\",\n               names_pattern = \"^p(.*)\",\n               values_to = \"values\")\n\nNow let’s say that we have the same data in a long format, using df2, but want to use it in a wide format.\n\ndf2 &lt;- df |&gt; \n  pivot_longer(cols = starts_with(\"p\"),\n               names_to = \"class\",\n               values_to = \"values\")\n\nhead(df2,5)\n\n# A tibble: 5 × 3\n  index class values\n  &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 p1      44.4\n2     1 p2      66.3\n3     1 p3      60.5\n4     2 p1      25.0\n5     2 p2      29.9\n\n\nIn this case, we use pivot_wider() and the names_from argument to specify which column will be used for the column names, and values_from for the values.\n\ndf2 |&gt; \n  pivot_wider(names_from = class, \n              values_from = values)\n\n# A tibble: 10 × 4\n   index    p1    p2    p3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 44.4   66.3  60.5\n 2     2 25.0   29.9  73.2\n 3     3  4.50  69.8  34.7\n 4     4 12.3   46.0  13.0\n 5     5 32.1   20.2  78.4\n 6     6 41.9   62.4  22.2\n 7     7 13.1   41.9  52.8\n 8     8 18.3   33.7  33.7\n 9     9 16.5   52.8  37.5\n10    10 26.5   63.3  28.3\n\n\n\n\nNext steps\nThe code above returns the simple tibble we started with but reshaping more complicated datasets might require the use of names_repair, names_sep, or names_expand arguments in pivot_wider. To learn about these and other arguments for both of the pivoting functions mentioned above then take a look at the detailed guide here. Happy pivoting!"
  }
]